{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "dd48470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"..\")\n",
    "from utils.train2 import *\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "833540a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_meta(os.path.join(os.pardir, 'metadata1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ed51e459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5902, 68)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "77208fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAtenttion(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, layers_list, activation=nn.ReLU(), dropout_list=None, batch_norm=True):\n",
    "        super(CustomAtenttion, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.layers_list = layers_list\n",
    "        self.activation = activation\n",
    "        self.dropout_list = dropout_list\n",
    "        self.batch_norm = batch_norm\n",
    "        self.net = []\n",
    "        self.b_list = []\n",
    "        if self.dropout_list:\n",
    "            self.dropout_list = [nn.Dropout(i) for i in self.dropout_list]\n",
    "        for i in range(len(self.layers_list)):\n",
    "            if self.batch_norm:\n",
    "                self.b_list.append(nn.BatchNorm1d(self.layers_list[i]))\n",
    "            if i==0:\n",
    "                self.net.append(nn.Linear(self.n_features, self.layers_list[i]))\n",
    "            else:\n",
    "                self.net.append(nn.Linear(self.layers_list[i-1]+3, self.layers_list[i]))\n",
    "        self.last_layer = nn.Linear(self.layers_list[-1]+self.n_features-3, self.n_classes)\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "        self.prev_atenttion = nn.Linear(self.n_features-3, 3*(self.n_features-3))\n",
    "        self.atenttion = nn.MultiheadAttention(self.n_features-3, 8, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b_size, _ = x.size()\n",
    "        vp = torch.clone(x[:,-3:])\n",
    "        embx = torch.clone(x[:,:-3])\n",
    "        embx = self.prev_atenttion(embx)\n",
    "        qkv = embx.view(b_size, 3, -1)\n",
    "        q, k, v = qkv.chunk(3, dim = 1)\n",
    "        out, _ = self.atenttion(q, k, v)\n",
    "        out = out.squeeze(dim=1)\n",
    "        for i, l in enumerate(self.net):\n",
    "            #print(i)\n",
    "            if i==0:\n",
    "                x = self.activation(l(x))\n",
    "            else:\n",
    "                x = self.activation(l(torch.cat((x, vp), dim=1)))\n",
    "            if self.batch_norm:\n",
    "                x = self.b_list[i](x)\n",
    "            if self.dropout_list:\n",
    "                x = (self.dropout_list[i])(x)\n",
    "            \n",
    "        x = self.last_layer(torch.cat((x, out), dim=1))\n",
    "        if self.n_classes == 1:\n",
    "            x = torch.sigmoid(x)\n",
    "   \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a3b0ca51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomAtenttion(\n",
      "  (activation): GELU()\n",
      "  (last_layer): Linear(in_features=320, out_features=788, bias=True)\n",
      "  (net): ModuleList(\n",
      "    (0): Linear(in_features=67, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=131, out_features=128, bias=True)\n",
      "    (2): Linear(in_features=131, out_features=256, bias=True)\n",
      "    (3): Linear(in_features=259, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=259, out_features=256, bias=True)\n",
      "  )\n",
      "  (prev_atenttion): Linear(in_features=64, out_features=192, bias=True)\n",
      "  (atenttion): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "mod = CustomAtenttion(67, 788, [128, 128, 256, 256, 256], nn.GELU(), batch_norm=False)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "51ca1c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdig\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "..\\utils\\train2.py:128: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \n",
      "C:\\Users\\abdig\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\model_selection\\_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n",
      "..\\utils\\train2.py:129: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  def train(data, model, ep = 120, save=False, prefix=None, device='cpu'):\n",
      "Acc train: 12.80 Acc test: 14.99:  18%|‚ñè| 9/50 [00:43<03:19\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-bfbcd7368fd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\UNI\\BELUGAS\\belugas_classification\\utils\\train2.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, model, ep, save, prefix, device)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mloss_epoch\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0macc_train\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\sale\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-87-3ddad14d3010>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_norm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\sale\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\sale\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    650\u001b[0m     \"\"\"\n\u001b[0;32m    651\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 652\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\sale\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mgelu\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_unary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eo = train(a, mod, ep=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af02f36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5757ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = nn.MultiheadAttention(12, 4, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47191b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m, w = at(torch.randn(1, 1, 12), torch.randn(1, 1, 12), torch.randn(1, 1, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d3ef5d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0836, -0.7130,  0.3655, -0.0394,  0.2155, -0.2543,  0.7236,\n",
       "           0.2701, -0.5202,  0.5084, -0.5707, -0.2636]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33165f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MultiheadAttention(2, 1, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d1e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial batch\n",
      "tensor([[ 1.0111, -0.4251, -1.0709, -0.5478,  0.7797,  0.4184],\n",
      "        [-0.4678, -0.7963,  0.8651, -1.5402,  0.0810,  0.5573],\n",
      "        [-1.5549,  0.1946, -0.5968,  1.2888,  0.2883, -0.6147],\n",
      "        [-1.2157,  0.4224,  0.0131, -0.6178,  0.5841,  0.9268]])\n",
      "Separadao por batch\n",
      "tensor([[[ 1.0111, -0.4251],\n",
      "         [-1.0709, -0.5478],\n",
      "         [ 0.7797,  0.4184]],\n",
      "\n",
      "        [[-0.4678, -0.7963],\n",
      "         [ 0.8651, -1.5402],\n",
      "         [ 0.0810,  0.5573]],\n",
      "\n",
      "        [[-1.5549,  0.1946],\n",
      "         [-0.5968,  1.2888],\n",
      "         [ 0.2883, -0.6147]],\n",
      "\n",
      "        [[-1.2157,  0.4224],\n",
      "         [ 0.0131, -0.6178],\n",
      "         [ 0.5841,  0.9268]]])\n",
      "q, k, v\n",
      "tensor([[[ 1.0111, -0.4251]],\n",
      "\n",
      "        [[-0.4678, -0.7963]],\n",
      "\n",
      "        [[-1.5549,  0.1946]],\n",
      "\n",
      "        [[-1.2157,  0.4224]]]) tensor([[[-1.0709, -0.5478]],\n",
      "\n",
      "        [[ 0.8651, -1.5402]],\n",
      "\n",
      "        [[-0.5968,  1.2888]],\n",
      "\n",
      "        [[ 0.0131, -0.6178]]]) tensor([[[ 0.7797,  0.4184]],\n",
      "\n",
      "        [[ 0.0810,  0.5573]],\n",
      "\n",
      "        [[ 0.2883, -0.6147]],\n",
      "\n",
      "        [[ 0.5841,  0.9268]]])\n",
      "salida\n",
      "tensor([[[ 0.0547, -0.1492]],\n",
      "\n",
      "        [[-0.1070, -0.1467]],\n",
      "\n",
      "        [[ 0.1890,  0.1412]],\n",
      "\n",
      "        [[-0.0936, -0.2684]]], grad_fn=<TransposeBackward0>)\n",
      "tensor([[ 0.0547, -0.1492],\n",
      "        [-0.1070, -0.1467],\n",
      "        [ 0.1890,  0.1412],\n",
      "        [-0.0936, -0.2684]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(4, 2*3)\n",
    "print(\"Initial batch\")\n",
    "print(a)\n",
    "b_size, _ = a.size()\n",
    "e = a.view(b_size, 3, -1)\n",
    "print(\"Separadao por batch\")\n",
    "print(e)\n",
    "q, k, v = e.chunk(3, dim = 1)\n",
    "print(\"q, k, v\")\n",
    "print(q, k, v)\n",
    "print(\"salida\")\n",
    "out, _ = m(q, k, v)\n",
    "print(out)\n",
    "print(out.squeeze(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "400eb464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
