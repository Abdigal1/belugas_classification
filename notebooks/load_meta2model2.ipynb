{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "94624647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "170356b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = os.path.join(os.pardir, 'metadata1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8f235432",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.kaiming_uniform_(m.weight)\n",
    "def get_columname(PATH):\n",
    "    for i in list(os.listdir(PATH)):\n",
    "        file = os.path.join(PATH, i)\n",
    "        instance = np.load(file, allow_pickle=True)\n",
    "        column = []\n",
    "        column += ['z_mu_' + str(j) for j in range(len(instance['z_mu']))]\n",
    "        column += ['z_sig_' + str(j) for j in range(len(instance['z_sig']))]\n",
    "        column += ['vp' + str(j) for j in range(len(instance['vp']))]\n",
    "        column.append('y')\n",
    "        return column\n",
    "\n",
    "def load_meta(PATH):\n",
    "    total = []\n",
    "    for i in list(os.listdir(PATH)):\n",
    "        instance_list= []\n",
    "        file = os.path.join(PATH, i)\n",
    "        instance = np.load(file, allow_pickle=True)\n",
    "        instance_list.extend(instance['z_mu'])\n",
    "        instance_list.extend(instance['z_sig'])\n",
    "        instance_list.extend(instance['vp'])\n",
    "        instance_list.append(int(instance['y'].item()))\n",
    "        total.append(instance_list)\n",
    "    \n",
    "    total = np.array(total)\n",
    "    df = pd.DataFrame(data = total, columns=get_columname(PATH))\n",
    "    df = df.astype({'vp0':int, 'vp1':int, 'vp2':int, 'y':int})\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0929ff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_meta(MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b178afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z_mu_0</th>\n",
       "      <th>z_mu_1</th>\n",
       "      <th>z_mu_2</th>\n",
       "      <th>z_mu_3</th>\n",
       "      <th>z_mu_4</th>\n",
       "      <th>z_mu_5</th>\n",
       "      <th>z_mu_6</th>\n",
       "      <th>z_mu_7</th>\n",
       "      <th>z_mu_8</th>\n",
       "      <th>z_mu_9</th>\n",
       "      <th>...</th>\n",
       "      <th>z_sig_26</th>\n",
       "      <th>z_sig_27</th>\n",
       "      <th>z_sig_28</th>\n",
       "      <th>z_sig_29</th>\n",
       "      <th>z_sig_30</th>\n",
       "      <th>z_sig_31</th>\n",
       "      <th>vp0</th>\n",
       "      <th>vp1</th>\n",
       "      <th>vp2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.618678</td>\n",
       "      <td>-0.658830</td>\n",
       "      <td>0.996242</td>\n",
       "      <td>-0.996403</td>\n",
       "      <td>-0.028649</td>\n",
       "      <td>-0.998194</td>\n",
       "      <td>0.999213</td>\n",
       "      <td>0.552009</td>\n",
       "      <td>-0.058491</td>\n",
       "      <td>-0.997051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.862571</td>\n",
       "      <td>-0.941453</td>\n",
       "      <td>0.955782</td>\n",
       "      <td>-0.997597</td>\n",
       "      <td>-0.208997</td>\n",
       "      <td>-0.999700</td>\n",
       "      <td>-0.987176</td>\n",
       "      <td>0.996988</td>\n",
       "      <td>-0.995751</td>\n",
       "      <td>-0.774342</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.785671</td>\n",
       "      <td>-0.859014</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>-0.997018</td>\n",
       "      <td>0.849262</td>\n",
       "      <td>-0.998977</td>\n",
       "      <td>-0.556846</td>\n",
       "      <td>0.984850</td>\n",
       "      <td>-0.441752</td>\n",
       "      <td>-0.999578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.931641</td>\n",
       "      <td>0.972322</td>\n",
       "      <td>0.999803</td>\n",
       "      <td>-0.864349</td>\n",
       "      <td>0.559585</td>\n",
       "      <td>-0.776151</td>\n",
       "      <td>0.964803</td>\n",
       "      <td>0.699126</td>\n",
       "      <td>0.312649</td>\n",
       "      <td>-0.999741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.080938</td>\n",
       "      <td>-0.685659</td>\n",
       "      <td>-0.165921</td>\n",
       "      <td>0.999313</td>\n",
       "      <td>0.963709</td>\n",
       "      <td>0.991040</td>\n",
       "      <td>-0.979468</td>\n",
       "      <td>-0.927676</td>\n",
       "      <td>-0.993806</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>-0.901259</td>\n",
       "      <td>-0.990732</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>-0.943246</td>\n",
       "      <td>0.819818</td>\n",
       "      <td>-0.606960</td>\n",
       "      <td>0.997472</td>\n",
       "      <td>-0.511044</td>\n",
       "      <td>-0.992544</td>\n",
       "      <td>-0.933740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>-0.998240</td>\n",
       "      <td>0.892293</td>\n",
       "      <td>0.794499</td>\n",
       "      <td>0.261753</td>\n",
       "      <td>-0.827686</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>0.993889</td>\n",
       "      <td>0.774129</td>\n",
       "      <td>0.987798</td>\n",
       "      <td>0.968225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>-0.911131</td>\n",
       "      <td>0.863759</td>\n",
       "      <td>0.999699</td>\n",
       "      <td>0.523237</td>\n",
       "      <td>0.146535</td>\n",
       "      <td>-0.946958</td>\n",
       "      <td>0.996871</td>\n",
       "      <td>0.538865</td>\n",
       "      <td>-0.276783</td>\n",
       "      <td>-0.939564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.993459</td>\n",
       "      <td>0.997001</td>\n",
       "      <td>-0.997850</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>-0.963214</td>\n",
       "      <td>0.484722</td>\n",
       "      <td>0.090078</td>\n",
       "      <td>0.981727</td>\n",
       "      <td>-0.999903</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>-0.941563</td>\n",
       "      <td>0.634128</td>\n",
       "      <td>-0.905059</td>\n",
       "      <td>-0.967807</td>\n",
       "      <td>-0.243647</td>\n",
       "      <td>0.998789</td>\n",
       "      <td>0.074211</td>\n",
       "      <td>-0.964556</td>\n",
       "      <td>-0.994068</td>\n",
       "      <td>0.809920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0.367879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5902 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        z_mu_0    z_mu_1    z_mu_2    z_mu_3    z_mu_4    z_mu_5    z_mu_6  \\\n",
       "0    -0.618678 -0.658830  0.996242 -0.996403 -0.028649 -0.998194  0.999213   \n",
       "1     0.862571 -0.941453  0.955782 -0.997597 -0.208997 -0.999700 -0.987176   \n",
       "2     0.785671 -0.859014  0.998854 -0.997018  0.849262 -0.998977 -0.556846   \n",
       "3    -0.931641  0.972322  0.999803 -0.864349  0.559585 -0.776151  0.964803   \n",
       "4    -0.080938 -0.685659 -0.165921  0.999313  0.963709  0.991040 -0.979468   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5897 -0.901259 -0.990732  0.999960 -0.943246  0.819818 -0.606960  0.997472   \n",
       "5898 -0.998240  0.892293  0.794499  0.261753 -0.827686 -0.065764  0.993889   \n",
       "5899 -0.911131  0.863759  0.999699  0.523237  0.146535 -0.946958  0.996871   \n",
       "5900  0.998635  0.993459  0.997001 -0.997850  0.999025 -0.963214  0.484722   \n",
       "5901 -0.941563  0.634128 -0.905059 -0.967807 -0.243647  0.998789  0.074211   \n",
       "\n",
       "        z_mu_7    z_mu_8    z_mu_9  ...  z_sig_26  z_sig_27  z_sig_28  \\\n",
       "0     0.552009 -0.058491 -0.997051  ...  0.367879  0.367879  0.367879   \n",
       "1     0.996988 -0.995751 -0.774342  ...  0.367879  0.367879  0.367879   \n",
       "2     0.984850 -0.441752 -0.999578  ...  0.367879  0.367879  0.367879   \n",
       "3     0.699126  0.312649 -0.999741  ...  0.367879  0.367879  0.367879   \n",
       "4    -0.927676 -0.993806  0.999467  ...  0.367879  0.367879  0.367879   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5897 -0.511044 -0.992544 -0.933740  ...  0.367879  0.367879  0.367879   \n",
       "5898  0.774129  0.987798  0.968225  ...  0.367879  0.367879  0.367879   \n",
       "5899  0.538865 -0.276783 -0.939564  ...  0.367879  0.367879  0.367879   \n",
       "5900  0.090078  0.981727 -0.999903  ...  0.367879  0.367879  0.367879   \n",
       "5901 -0.964556 -0.994068  0.809920  ...  0.367879  0.367879  0.367879   \n",
       "\n",
       "      z_sig_29  z_sig_30  z_sig_31  vp0  vp1  vp2    y  \n",
       "0     0.367879  0.367879  0.367879    0    0    1    0  \n",
       "1     0.367879  0.367879  0.367879    0    0    1   10  \n",
       "2     0.367879  0.367879  0.367879    0    0    1  130  \n",
       "3     0.367879  0.367879  0.367879    0    0    1   72  \n",
       "4     0.367879  0.367879  0.367879    0    0    1  413  \n",
       "...        ...       ...       ...  ...  ...  ...  ...  \n",
       "5897  0.367879  0.367879  0.367879    0    0    1   75  \n",
       "5898  0.367879  0.367879  0.367879    0    0    1  108  \n",
       "5899  0.367879  0.367879  0.367879    0    0    1  354  \n",
       "5900  0.367879  0.367879  0.367879    1    0    0   83  \n",
       "5901  0.367879  0.367879  0.367879    0    0    1    9  \n",
       "\n",
       "[5902 rows x 68 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd473db",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d154820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "055f443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ed6e987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([ 93,  15,  66,  10,  62,  36,  82, 189,  28,  77,\n",
       "            ...\n",
       "            671, 677, 678, 682, 207, 683, 685, 209, 687, 400],\n",
       "           dtype='int64', length=788)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b060b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = [k for k in list(b.keys()) if b[k]>=6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a30456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = a[a['y'].isin(nl)]\n",
    "ndf_2 = ndf.replace({'y':{item:i for i, item in enumerate(np.unique(ndf.y.values))}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9329cf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, y = a.iloc[:,:-1].values, a.iloc[:,-1].values\n",
    "X, y = ndf_2.iloc[:,:-1].values, ndf_2.iloc[:,-1].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9247bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "611d492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              eval_metric='mlogloss', gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=100, n_jobs=8,\n",
       "              num_parallel_tree=1, objective='multi:softprob', predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, ...)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2124b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e891f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0021141649048625794"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801361a2",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cc4201af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5902, 68)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7cc48f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "788\n"
     ]
    }
   ],
   "source": [
    "lol = sorted(list(a.y.unique()))\n",
    "print(len(lol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "36873e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class CustomTrain(nn.Module):\n",
    "    def __init__(self, n_features, n_classes, layers_list, activation=nn.ReLU(), dropout_list=None, batch_norm=True):\n",
    "        super(CustomTrain, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.n_classes = n_classes\n",
    "        self.layers_list = layers_list\n",
    "        self.activation = activation\n",
    "        self.dropout_list = dropout_list\n",
    "        self.batch_norm = batch_norm\n",
    "        self.net = []\n",
    "        self.b_list = []\n",
    "        if self.dropout_list:\n",
    "            self.dropout_list = [nn.Dropout(i) for i in self.dropout_list]\n",
    "        for i in range(len(self.layers_list)):\n",
    "            if self.batch_norm:\n",
    "                self.b_list.append(nn.BatchNorm1d(self.layers_list[i]))\n",
    "            if i==0:\n",
    "                self.net.append(nn.Linear(self.n_features, self.layers_list[i]))\n",
    "            else:\n",
    "                self.net.append(nn.Linear(self.layers_list[i-1], self.layers_list[i]))\n",
    "        self.last_layer = nn.Linear(self.layers_list[-1], self.n_classes)\n",
    "        self.net = nn.ModuleList(self.net)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, l in enumerate(self.net):\n",
    "            #print(i)\n",
    "            x.to(device)\n",
    "            x = self.activation(l(x))\n",
    "            if self.batch_norm:\n",
    "                x = self.b_list[i](x)\n",
    "            if self.dropout_list:\n",
    "                x = (self.dropout_list[i])(x)\n",
    "            \n",
    "        x = self.last_layer(x)\n",
    "        if self.n_classes == 1:\n",
    "            x = torch.sigmoid(x)\n",
    "   \n",
    "        return x\n",
    "\n",
    "    def eval(self):\n",
    "        self.train(False)\n",
    "        for i in self.b_list:\n",
    "            i.train(False)\n",
    "\n",
    "    def cust_train(self):\n",
    "        self.train(True)\n",
    "        for i in self.b_list:\n",
    "            i.train(True)\n",
    "        \n",
    "\n",
    "\n",
    "class datapaltas(Dataset):\n",
    "    def __init__(self, df, scale =True, y_idx=-1):\n",
    "        self.df = df\n",
    "        self.y_idx = y_idx\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = (self.df.iloc[index,:self.y_idx]).values\n",
    "        X = X.astype(np.float64)\n",
    "        X = torch.from_numpy(X).float()\n",
    "        y = self.df.iloc[index,self.y_idx]\n",
    "        #print(y)\n",
    "        y = torch.Tensor([y]).long()\n",
    "        return {'x':X, 'y':y}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6e252c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datapaltas(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1575c5fc-4d7e-4352-bc02-909d690a4925",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6ec1138d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "def train(data, model, ep = 120, save=False, prefix=None):\n",
    "    idx_result = {}\n",
    "    a = datapaltas(df = data, y_idx=-1)\n",
    "    EPOCHS = ep\n",
    "    skf = StratifiedKFold(shuffle=True)\n",
    "    ## y_idx -2\n",
    "    skf.get_n_splits(data, data.iloc[:,-1])\n",
    "    vec_train = np.array([train_ids for train_ids,_ in skf.split(data, data.iloc[:,-1])])\n",
    "    vec_test = np.array([test_ids for _,test_ids in skf.split(data, data.iloc[:,-1])])\n",
    "    acv, tav = [], []\n",
    "    foldn = 0\n",
    "    for train_ids, test_ids in zip(vec_train, vec_test):\n",
    "        acv_, tav_ = [], []\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "                            a, \n",
    "                            batch_size=64, sampler=train_subsampler)\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "                            a,\n",
    "                            batch_size=1, sampler=test_subsampler)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "        EPOCHS = ep\n",
    "        itt = tqdm(range(EPOCHS))\n",
    "        for i in itt:\n",
    "            loss_epoch = 0\n",
    "            acc_train, acc_test = 0.0, 0.0\n",
    "            model.cust_train()\n",
    "            for sample in train_loader:\n",
    "                X, y = sample['x'], sample['y']\n",
    "                X, y = X.to(device), y.to(device).flatten()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X)\n",
    "                #print(y_pred, y)\n",
    "                loss = criterion(y_pred, y)\n",
    "                acc = multi_acc(y_pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_epoch += loss.item()\n",
    "                acc_train += acc.item()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for sample in test_loader:\n",
    "                    X, y = sample['x'], sample['y']\n",
    "                    X, y = X.to(device), y.to(device).flatten()\n",
    "                    model.eval()\n",
    "                    output = model(X)\n",
    "                    acc = multi_acc(output, y)\n",
    "                    acc_test += acc.item()\n",
    "                \n",
    "            acv_.append(acc_test/len(test_loader))\n",
    "            tav_.append(acc_train/len(train_loader))\n",
    "            #print(acv_)\n",
    "            if save and (abs(acc_test/len(test_loader) - max(acv_)<1e-8)):\n",
    "                    if os.path.exists(os.path.join(os.curdir, prefix+\"model_fold_\"+str(foldn)+\".pth\")):\n",
    "                        os.remove(os.path.join(os.curdir, prefix+\"model_fold_\"+str(foldn)+\".pth\"))\n",
    "                    torch.save(model, os.path.join(os.curdir, prefix+\"model_fold_\"+str(foldn)+\".pth\"))\n",
    "                    itt.set_postfix({'epoch_model': i, 'best_acc':max(acv_)})\n",
    "            t1, t2 = acc_train/len(train_loader), acc_test/len(test_loader)\n",
    "            itt.set_description(f\"Acc train: {t1:.2f} Acc test: {t2:.2f}\")\n",
    "        acv.append(acv_)\n",
    "        tav.append(tav_)\n",
    "        \n",
    "\n",
    "        foldn += 1   \n",
    "        model.apply(init_normal)\n",
    "    acv = np.array(acv)\n",
    "    tav = np.array(tav)\n",
    "    idx_result['acc'] = np.mean(acv, axis = 0)\n",
    "    idx_result['acc_std'] = np.std(acv, axis=0)\n",
    "    idx_result['train_acc'] = np.mean(tav, axis=0)\n",
    "    return idx_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "507111f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTrain(\n",
       "  (activation): GELU()\n",
       "  (last_layer): Linear(in_features=256, out_features=788, bias=True)\n",
       "  (net): ModuleList(\n",
       "    (0): Linear(in_features=67, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = CustomTrain(67, 788, [1024, 1024, 512, 256], nn.GELU(), batch_norm=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "c27cc97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\paltas\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_5468/1134014984.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vec_train = np.array([train_ids for train_ids,_ in skf.split(data, data.iloc[:,-1])])\n",
      "C:\\Users\\LENOVO\\.conda\\envs\\paltas\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_5468/1134014984.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vec_test = np.array([test_ids for _,test_ids in skf.split(data, data.iloc[:,-1])])\n",
      "Acc train: 98.85 Acc test: 76.12: 100%|███████████████| 100/100 [07:01<00:00,  4.22s/it, epoch_model=99, best_acc=77.5]\n",
      "Acc train: 94.23 Acc test: 75.78: 100%|███████████████| 100/100 [07:52<00:00,  4.73s/it, epoch_model=99, best_acc=78.7]\n",
      "Acc train: 97.61 Acc test: 76.78: 100%|███████████████| 100/100 [07:52<00:00,  4.72s/it, epoch_model=99, best_acc=78.1]\n",
      "Acc train: 97.54 Acc test: 77.03: 100%|███████████████| 100/100 [07:52<00:00,  4.72s/it, epoch_model=99, best_acc=78.1]\n",
      "Acc train: 97.73 Acc test: 76.19: 100%|███████████████| 100/100 [07:13<00:00,  4.33s/it, epoch_model=99, best_acc=78.1]\n"
     ]
    }
   ],
   "source": [
    "eo = train(a, model, ep=100, save=True, prefix=\"met_1_batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35f62806-9285-4b81-b25b-aa2885264a2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x202d9c6f5e0>]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(eo['acc'])\n",
    "plt.plot(eo['train_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af80210-1214-4112-a8e0-38ab2315c5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dcb0a1b9-f928-409c-9a21-17d6acb4c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MP = os.path.join(os.pardir, 'metadata_50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9115033d-fa3b-4871-a799-cec6e232e52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = load_meta(MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b73d0f74-2f92-4d41-9b49-75f63e2fcb3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>z_mu_0</th>\n",
       "      <th>z_mu_1</th>\n",
       "      <th>z_mu_2</th>\n",
       "      <th>z_mu_3</th>\n",
       "      <th>z_mu_4</th>\n",
       "      <th>z_mu_5</th>\n",
       "      <th>z_mu_6</th>\n",
       "      <th>z_mu_7</th>\n",
       "      <th>z_mu_8</th>\n",
       "      <th>z_mu_9</th>\n",
       "      <th>...</th>\n",
       "      <th>z_sig_122</th>\n",
       "      <th>z_sig_123</th>\n",
       "      <th>z_sig_124</th>\n",
       "      <th>z_sig_125</th>\n",
       "      <th>z_sig_126</th>\n",
       "      <th>z_sig_127</th>\n",
       "      <th>vp0</th>\n",
       "      <th>vp1</th>\n",
       "      <th>vp2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168592</td>\n",
       "      <td>-0.169763</td>\n",
       "      <td>1.123454</td>\n",
       "      <td>-0.102161</td>\n",
       "      <td>-0.123310</td>\n",
       "      <td>-0.047843</td>\n",
       "      <td>-0.164133</td>\n",
       "      <td>-0.031004</td>\n",
       "      <td>0.544314</td>\n",
       "      <td>-0.154607</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845943</td>\n",
       "      <td>0.918356</td>\n",
       "      <td>0.845117</td>\n",
       "      <td>1.013998</td>\n",
       "      <td>0.844486</td>\n",
       "      <td>0.843885</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.134099</td>\n",
       "      <td>-0.169959</td>\n",
       "      <td>0.483605</td>\n",
       "      <td>-0.147921</td>\n",
       "      <td>0.099595</td>\n",
       "      <td>0.387963</td>\n",
       "      <td>0.275314</td>\n",
       "      <td>-0.147721</td>\n",
       "      <td>0.811916</td>\n",
       "      <td>0.018589</td>\n",
       "      <td>...</td>\n",
       "      <td>0.847346</td>\n",
       "      <td>0.886576</td>\n",
       "      <td>0.845728</td>\n",
       "      <td>0.855688</td>\n",
       "      <td>0.844057</td>\n",
       "      <td>0.847223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.112607</td>\n",
       "      <td>-0.156923</td>\n",
       "      <td>0.352698</td>\n",
       "      <td>-0.086046</td>\n",
       "      <td>-0.167899</td>\n",
       "      <td>1.340568</td>\n",
       "      <td>0.360664</td>\n",
       "      <td>-0.152542</td>\n",
       "      <td>1.142988</td>\n",
       "      <td>-0.168620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.888304</td>\n",
       "      <td>0.851786</td>\n",
       "      <td>0.851824</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>0.850179</td>\n",
       "      <td>0.844186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.093404</td>\n",
       "      <td>-0.169841</td>\n",
       "      <td>0.496092</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>-0.163540</td>\n",
       "      <td>1.136659</td>\n",
       "      <td>-0.140916</td>\n",
       "      <td>-0.151269</td>\n",
       "      <td>1.058258</td>\n",
       "      <td>-0.156523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.865980</td>\n",
       "      <td>0.844812</td>\n",
       "      <td>0.849311</td>\n",
       "      <td>0.844281</td>\n",
       "      <td>0.873991</td>\n",
       "      <td>0.850026</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.079154</td>\n",
       "      <td>1.011713</td>\n",
       "      <td>-0.154259</td>\n",
       "      <td>1.253120</td>\n",
       "      <td>0.797030</td>\n",
       "      <td>-0.134991</td>\n",
       "      <td>0.149675</td>\n",
       "      <td>0.755909</td>\n",
       "      <td>-0.157246</td>\n",
       "      <td>1.036069</td>\n",
       "      <td>...</td>\n",
       "      <td>4.220757</td>\n",
       "      <td>4.427835</td>\n",
       "      <td>4.969017</td>\n",
       "      <td>3.742840</td>\n",
       "      <td>3.285074</td>\n",
       "      <td>3.491657</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>-0.074372</td>\n",
       "      <td>-0.149521</td>\n",
       "      <td>0.292414</td>\n",
       "      <td>-0.024752</td>\n",
       "      <td>-0.152286</td>\n",
       "      <td>-0.167468</td>\n",
       "      <td>-0.169643</td>\n",
       "      <td>-0.167983</td>\n",
       "      <td>0.733661</td>\n",
       "      <td>-0.162364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.936005</td>\n",
       "      <td>1.122033</td>\n",
       "      <td>0.858926</td>\n",
       "      <td>1.424070</td>\n",
       "      <td>0.844585</td>\n",
       "      <td>0.848149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>-0.164903</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>-0.168730</td>\n",
       "      <td>-0.138961</td>\n",
       "      <td>-0.167916</td>\n",
       "      <td>0.152879</td>\n",
       "      <td>0.197204</td>\n",
       "      <td>-0.163089</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>-0.157118</td>\n",
       "      <td>...</td>\n",
       "      <td>1.118588</td>\n",
       "      <td>0.871303</td>\n",
       "      <td>0.878870</td>\n",
       "      <td>1.070225</td>\n",
       "      <td>0.936464</td>\n",
       "      <td>0.917033</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>-0.017157</td>\n",
       "      <td>-0.168093</td>\n",
       "      <td>0.344460</td>\n",
       "      <td>-0.118861</td>\n",
       "      <td>-0.137622</td>\n",
       "      <td>0.513425</td>\n",
       "      <td>-0.122537</td>\n",
       "      <td>-0.155546</td>\n",
       "      <td>0.901674</td>\n",
       "      <td>-0.039641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.844272</td>\n",
       "      <td>0.843689</td>\n",
       "      <td>0.844158</td>\n",
       "      <td>0.985738</td>\n",
       "      <td>0.856115</td>\n",
       "      <td>0.846153</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>0.354408</td>\n",
       "      <td>-0.157212</td>\n",
       "      <td>-0.135073</td>\n",
       "      <td>-0.167457</td>\n",
       "      <td>0.510792</td>\n",
       "      <td>0.190302</td>\n",
       "      <td>-0.049607</td>\n",
       "      <td>-0.169857</td>\n",
       "      <td>0.860662</td>\n",
       "      <td>-0.086137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.870772</td>\n",
       "      <td>0.855060</td>\n",
       "      <td>0.844191</td>\n",
       "      <td>0.917085</td>\n",
       "      <td>0.855870</td>\n",
       "      <td>0.871705</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5901</th>\n",
       "      <td>-0.109937</td>\n",
       "      <td>-0.053592</td>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.674239</td>\n",
       "      <td>0.036554</td>\n",
       "      <td>-0.160730</td>\n",
       "      <td>-0.120265</td>\n",
       "      <td>1.122864</td>\n",
       "      <td>-0.159824</td>\n",
       "      <td>1.605727</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256022</td>\n",
       "      <td>1.472374</td>\n",
       "      <td>1.631266</td>\n",
       "      <td>2.050417</td>\n",
       "      <td>0.872481</td>\n",
       "      <td>1.313050</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5902 rows × 260 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        z_mu_0    z_mu_1    z_mu_2    z_mu_3    z_mu_4    z_mu_5    z_mu_6  \\\n",
       "0    -0.168592 -0.169763  1.123454 -0.102161 -0.123310 -0.047843 -0.164133   \n",
       "1    -0.134099 -0.169959  0.483605 -0.147921  0.099595  0.387963  0.275314   \n",
       "2    -0.112607 -0.156923  0.352698 -0.086046 -0.167899  1.340568  0.360664   \n",
       "3    -0.093404 -0.169841  0.496092 -0.163298 -0.163540  1.136659 -0.140916   \n",
       "4     0.079154  1.011713 -0.154259  1.253120  0.797030 -0.134991  0.149675   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5897 -0.074372 -0.149521  0.292414 -0.024752 -0.152286 -0.167468 -0.169643   \n",
       "5898 -0.164903  0.038168 -0.168730 -0.138961 -0.167916  0.152879  0.197204   \n",
       "5899 -0.017157 -0.168093  0.344460 -0.118861 -0.137622  0.513425 -0.122537   \n",
       "5900  0.354408 -0.157212 -0.135073 -0.167457  0.510792  0.190302 -0.049607   \n",
       "5901 -0.109937 -0.053592  0.099576  0.674239  0.036554 -0.160730 -0.120265   \n",
       "\n",
       "        z_mu_7    z_mu_8    z_mu_9  ...  z_sig_122  z_sig_123  z_sig_124  \\\n",
       "0    -0.031004  0.544314 -0.154607  ...   0.845943   0.918356   0.845117   \n",
       "1    -0.147721  0.811916  0.018589  ...   0.847346   0.886576   0.845728   \n",
       "2    -0.152542  1.142988 -0.168620  ...   0.888304   0.851786   0.851824   \n",
       "3    -0.151269  1.058258 -0.156523  ...   0.865980   0.844812   0.849311   \n",
       "4     0.755909 -0.157246  1.036069  ...   4.220757   4.427835   4.969017   \n",
       "...        ...       ...       ...  ...        ...        ...        ...   \n",
       "5897 -0.167983  0.733661 -0.162364  ...   0.936005   1.122033   0.858926   \n",
       "5898 -0.163089  0.515000 -0.157118  ...   1.118588   0.871303   0.878870   \n",
       "5899 -0.155546  0.901674 -0.039641  ...   0.844272   0.843689   0.844158   \n",
       "5900 -0.169857  0.860662 -0.086137  ...   0.870772   0.855060   0.844191   \n",
       "5901  1.122864 -0.159824  1.605727  ...   1.256022   1.472374   1.631266   \n",
       "\n",
       "      z_sig_125  z_sig_126  z_sig_127  vp0  vp1  vp2    y  \n",
       "0      1.013998   0.844486   0.843885    0    0    1    0  \n",
       "1      0.855688   0.844057   0.847223    0    0    1   10  \n",
       "2      0.906000   0.850179   0.844186    0    0    1  130  \n",
       "3      0.844281   0.873991   0.850026    0    0    1   72  \n",
       "4      3.742840   3.285074   3.491657    0    0    1  413  \n",
       "...         ...        ...        ...  ...  ...  ...  ...  \n",
       "5897   1.424070   0.844585   0.848149    0    0    1   75  \n",
       "5898   1.070225   0.936464   0.917033    0    0    1  108  \n",
       "5899   0.985738   0.856115   0.846153    0    0    1  354  \n",
       "5900   0.917085   0.855870   0.871705    1    0    0   83  \n",
       "5901   2.050417   0.872481   1.313050    0    0    1    9  \n",
       "\n",
       "[5902 rows x 260 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0ea1bd0b-64d2-48d9-b65c-4bb95ee429fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomTrain(\n",
       "  (activation): GELU()\n",
       "  (last_layer): Linear(in_features=256, out_features=788, bias=True)\n",
       "  (net): ModuleList(\n",
       "    (0): Linear(in_features=259, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = CustomTrain(259, 788, [1024, 1024, 512, 256], nn.GELU(), batch_norm=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b23f880-dddf-4a0b-9c85-56d598916673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\.conda\\envs\\paltas\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_5468/3585931085.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vec_train = np.array([train_ids for train_ids,_ in skf.split(data, data.iloc[:,-1])])\n",
      "C:\\Users\\LENOVO\\.conda\\envs\\paltas\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp/ipykernel_5468/3585931085.py:10: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vec_test = np.array([test_ids for _,test_ids in skf.split(data, data.iloc[:,-1])])\n",
      "Acc train: 94.20 Acc test: 6.27: 100%|███████████████| 200/200 [15:14<00:00,  4.57s/it, epoch_model=199, best_acc=7.71]\n",
      "Acc train: 95.76 Acc test: 14.39: 100%|██████████████| 200/200 [15:32<00:00,  4.66s/it, epoch_model=199, best_acc=81.8]\n",
      "Acc train: 97.19 Acc test: 29.83: 100%|██████████████| 200/200 [16:16<00:00,  4.88s/it, epoch_model=199, best_acc=95.3]\n",
      "Acc train: 96.32 Acc test: 46.36: 100%|██████████████| 200/200 [16:31<00:00,  4.96s/it, epoch_model=199, best_acc=98.7]\n",
      "Acc train: 98.03 Acc test: 55.00: 100%|██████████████| 200/200 [16:47<00:00,  5.04s/it, epoch_model=199, best_acc=99.2]\n"
     ]
    }
   ],
   "source": [
    "eo = train(a, model, ep=200, save=True, prefix='met_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb7427-c6d6-41c7-b1ba-01011b0cdf48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paltas",
   "language": "python",
   "name": "paltas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
